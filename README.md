This project contains experiments with an energy modulated dropout algorithm.

The idea is for the new dropout scheme to learn less correlated functions.
And therefore cover a bigger region of function space at each layer.
Less bias with the same number of parameters.

Another interesting angle is to think of this as a self-attention mechanism for backprop.
This line of thinking could actually shine a different light on what attention actually is.
